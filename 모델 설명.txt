일반적으로 한 레이어에서의 커널의 수를 늘리는 것보단 깊게 모델링하는것이 성능이 좋다고 알려져 있음.

따라서 한 레이어당 커널 갯수는 최대 128개까지로 제한함 (256, 512...로 너무 많아지지않게)

레이어당 커널의 수가 2의 n승으로 구성된 건 워드 및 메모리 단위의 n배로 맞추기 위해서

하지만 모델이 깊어지면 파라미터들의 전달이 제대로 되지 않는 문제 발생(기울기 소실 등)

이를 방지하기 위해서 모든 레이어 입력 앞에서 batch normalization을 수행

또한 학습 효과 향상을 위해 모든 레이어 입력 앞에 dropout 적용. 비율은 25%

batch normalization -> dropout 순서로 적용. 이 순서가 dropout의 기본 원리에 가깝다고 생각함

optimizer나 loss함수는 cnn모델에서는 이걸 많이 쓰길래 썼음